{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the front page of some always-updating website (some websites won't work with BeautifulSoup).\n",
    "\n",
    "Send yourself an email with as much information as possible from the site, such as:  \n",
    "\n",
    " - The title of the thing (the sale, the article, whatever)\n",
    " - A URL for it\n",
    " - Upvotes/thumbs ups/subreddits/prices/links to images/etc\n",
    "\n",
    "Save this as a CSV, and send it as an attachment to your email address every 6 hours. The email headline should say something like \"**Here is your 6PM briefing.**\" The CSV file should be timestamped with the current date and time, e.g. `briefing-2018-06-18-3PM.csv`\n",
    "\n",
    "**BONUS**: Have the content actually be the **body** of the email, not just an attachment. I don't mean like a CSV or whatever, I mean it should actually look like nice lists and stuff, a real email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this assignment, I'm scraping China-related news from Google News.\n",
    "response = requests.get(\"https://news.google.com/search?q=China&hl=en-US&gl=US&ceid=US%3Aen\").content\n",
    "soup = BeautifulSoup(response, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before we put the data into a df, we use print() to make sure our program works\n",
    "\n",
    "# The first two blocks are featured article groups (with 'View more' buttons).\n",
    "# They need to be dealt with separately.\n",
    "# And since we don't know if the number of featured blocks is gonna change,\n",
    "# we have to find the uniqueness of these blocks.\n",
    "# I'm only taking the first article of each group for simplicity.\n",
    "\n",
    "# It could be observed that the featured blocks have an attr 'jscontroller', whose value is d0DtYd\n",
    "featured = soup.find(class_='lBwEZb BL5WZb xP6mwf').findAll('div',attrs={'jscontroller':'d0DtYd'})\n",
    "for feature in featured:\n",
    "    feature_article = feature.find('article').find(class_='ZulkBc qNiaOd')\n",
    "    feature_title = feature_article.span.text\n",
    "    feature_url = 'https://news.google.com'+feature_article.a['href'][1:]\n",
    "    feature_first_lines = feature_article.find(class_='HO8did Baotjf').text\n",
    "    feature_source = feature.find(class_='QmrVtf kybdz').find('div',attrs={'class':'PNwZO zhsNkd'}).text\n",
    "    feature_rawtime = feature.find('time')['datetime'].split(': ')[1]\n",
    "    feature_time = datetime.datetime.fromtimestamp(int(feature_rawtime)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print('-------')\n",
    "    print(feature_title)\n",
    "    print(feature_url)\n",
    "    print(feature_first_lines)\n",
    "    print(feature_source)\n",
    "    print(feature_time)\n",
    "\n",
    "# The other \"normal\" articles have an attr 'jsmodel', whose value is 'zT6vwb'\n",
    "articles = soup.find(class_='lBwEZb BL5WZb xP6mwf').findAll('div',attrs={'jsmodel':'zT6vwb'})\n",
    "\n",
    "for article in articles:\n",
    "    article_title = article.find('a',attrs={'class':'ipQwMb Q7tWef'}).span.text\n",
    "    article_url = 'https://news.google.com'+article.find('a',attrs={'class':'ipQwMb Q7tWef'})['href'][1:]\n",
    "    article_first_lines = article.find('p').text\n",
    "    article_source = article.find(class_='KbnJ8').text\n",
    "    article_rawtime = re.findall(r'[\\d]+', article.find('time')['datetime'].split(': ')[1])[0]\n",
    "    article_time = datetime.datetime.fromtimestamp(int(article_rawtime)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print('-------')\n",
    "    print(article_title)\n",
    "    print(article_url)\n",
    "    print(article_first_lines)\n",
    "    print(article_source)\n",
    "    print(article_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we put everything together  \n",
    "Put the data into a df, save it as a CSV, then send as attachment using mailgun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating auto-emails on China-related news from Google News\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://news.google.com/search?q=China&hl=en-US&gl=US&ceid=US%3Aen\").content\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "briefing = []\n",
    "\n",
    "featured = soup.find(class_='lBwEZb BL5WZb xP6mwf').findAll('div',attrs={'jscontroller':'d0DtYd'})\n",
    "for feature in featured:\n",
    "    feature_row = {}\n",
    "    feature_article = feature.find('article').find(class_='ZulkBc qNiaOd')\n",
    "    feature_row['title'] = feature_article.span.text\n",
    "    feature_row['url'] = 'https://news.google.com'+feature_article.a['href'][1:]\n",
    "    feature_row['first_lines'] = feature_article.find(class_='HO8did Baotjf').text\n",
    "    feature_row['source'] = feature.find(class_='QmrVtf kybdz').find('div',attrs={'class':'PNwZO zhsNkd'}).text\n",
    "    feature_rawtime = feature.find('time')['datetime'].split(': ')[1]\n",
    "    feature_row['time'] = datetime.datetime.fromtimestamp(int(feature_rawtime)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    briefing.append(feature_row)\n",
    "\n",
    "articles = soup.find(class_='lBwEZb BL5WZb xP6mwf').findAll('div',attrs={'jsmodel':'zT6vwb'})\n",
    "for article in articles:\n",
    "    article_row = {}\n",
    "    article_row['title'] = article.find('a',attrs={'class':'ipQwMb Q7tWef'}).span.text\n",
    "    article_row['url'] = 'https://news.google.com'+article.find('a',attrs={'class':'ipQwMb Q7tWef'})['href'][1:]\n",
    "    article_row['first_lines'] = article.find('p').text\n",
    "    article_row['source'] = article.find(class_='KbnJ8').text\n",
    "    article_rawtime = re.findall(r'[\\d]+', article.find('time')['datetime'].split(': ')[1])[0]\n",
    "    article_row['time'] = datetime.datetime.fromtimestamp(int(article_rawtime)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    briefing.append(article_row)\n",
    "    \n",
    "df = pd.DataFrame(briefing)\n",
    "right_now = datetime.datetime.now()\n",
    "date_string_filename = right_now.strftime(\"%Y-%b-%d_%-I%p\")\n",
    "df.to_csv('China_news_briefing_{}.csv'.format(date_string_filename), index=False)\n",
    "date_string_mail = right_now.strftime(\"%-I %p\")\n",
    "\n",
    "requests.post(\n",
    "        \"https://api.mailgun.net/v3/MY_SANDBOX_DOMAIN/messages\",\n",
    "        auth=(\"api\", \"MY_API_KEY\"),\n",
    "        files=[(\"attachment\", open('China_news_briefing_{}.csv'.format(date_string_filename)))],\n",
    "        data={\"from\": \"Edward Hong <mailgun@MY_SANDBOX_DOMAIN>\",\n",
    "              \"to\": [\"Edward.YSHF@gmail.com\"],\n",
    "              \"subject\": \"{} China News Briefing\".format(date_string_mail),\n",
    "              \"text\": \"See attachment to learn what's new about China.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating auto-emails on China-related news from Google News\n",
    "# BONUS: presenting the content not as an CSV attachment but as the body of email\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "response = requests.get(\"https://news.google.com/search?q=China&hl=en-US&gl=US&ceid=US%3Aen\").content\n",
    "soup = BeautifulSoup(response, \"html.parser\")\n",
    "\n",
    "briefing = []\n",
    "\n",
    "featured = soup.find(class_='lBwEZb BL5WZb xP6mwf').findAll('div',attrs={'jscontroller':'d0DtYd'})\n",
    "for feature in featured:\n",
    "    feature_row = {}\n",
    "    feature_article = feature.find('article').find(class_='ZulkBc qNiaOd')\n",
    "    feature_row['title'] = feature_article.span.text\n",
    "    feature_row['url'] = 'https://news.google.com'+feature_article.a['href'][1:]\n",
    "    feature_row['first_lines'] = feature_article.find(class_='HO8did Baotjf').text\n",
    "    feature_row['source'] = feature.find(class_='QmrVtf kybdz').find('div',attrs={'class':'PNwZO zhsNkd'}).text\n",
    "    feature_rawtime = feature.find('time')['datetime'].split(': ')[1]\n",
    "    feature_row['time'] = datetime.datetime.fromtimestamp(int(feature_rawtime)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    briefing.append(feature_row)\n",
    "\n",
    "articles = soup.find(class_='lBwEZb BL5WZb xP6mwf').findAll('div',attrs={'jsmodel':'zT6vwb'})\n",
    "for article in articles:\n",
    "    article_row = {}\n",
    "    article_row['title'] = article.find('a',attrs={'class':'ipQwMb Q7tWef'}).span.text\n",
    "    article_row['url'] = 'https://news.google.com'+article.find('a',attrs={'class':'ipQwMb Q7tWef'})['href'][1:]\n",
    "    article_row['first_lines'] = article.find('p').text\n",
    "    article_row['source'] = article.find(class_='KbnJ8').text\n",
    "    article_rawtime = re.findall(r'[\\d]+', article.find('time')['datetime'].split(': ')[1])[0]\n",
    "    article_row['time'] = datetime.datetime.fromtimestamp(int(article_rawtime)).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    briefing.append(article_row)\n",
    "    \n",
    "df = pd.DataFrame(briefing)\n",
    "right_now = datetime.datetime.now()\n",
    "date_string_mail = right_now.strftime(\"%-I %p\")\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "requests.post(\n",
    "        \"https://api.mailgun.net/v3/MY_SANDBOX_DOMAIN/messages\",\n",
    "        auth=(\"api\", \"MY_API_KEY\"),\n",
    "        data={\"from\": \"Edward Hong <mailgun@MY_SANDBOX_DOMAIN>\",\n",
    "              \"to\": [\"Edward.YSHF@gmail.com\"],\n",
    "              \"subject\": \"{} China News Briefing\".format(date_string_mail),\n",
    "              \"html\": df.to_html()})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
